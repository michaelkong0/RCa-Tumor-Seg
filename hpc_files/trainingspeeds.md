#### training speeds
Generally use 8 to 10 CPUs per training and ~50 GB per training (if possible)

V100: ~65 seconds per epoch -- if training more than one model at once on separate GPUs
2080: ~120 seconds per epoch
P100: ~600-700 seconds per epoch